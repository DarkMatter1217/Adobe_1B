# ğŸ³ Run Adobe Hackathon 1B Solution using Docker

## ğŸ“† About the Project

This Dockerized project solves the **Adobe Hackathon Round 1B** problem:

> Persona-driven document analysis â€” extract & summarize the most relevant sections based on a user persona and task.

Technologies used:

* ğŸ¦© TinyLLaMA (via `llama-cpp-python`)
* ğŸ”— LangChain
* ğŸ¤— HuggingFace Embeddings
* ğŸ§  FAISS Vector Store
  Runs fully **offline** using Docker.

---

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ dockerfile                 # Docker instructions
â”œâ”€â”€ .dockerignore              # Ignore output/models etc. during docker builds
â”œâ”€â”€ solution.py                # Main script
â”œâ”€â”€ requirements.txt           # All Python dependencies
â”œâ”€â”€ input/
â”‚   â”œâ”€â”€ input.json             # Contains persona, task, and document info
â”‚   â””â”€â”€ <document>.pdf/html    # Supporting documents
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tinyllama/             # Quantized TinyLLaMA model
â”‚   â””â”€â”€ bge-small-en-v1.5/     # Embedding model folder
â”œâ”€â”€ output/                    # Auto-created for results
```

---

## ğŸ› ï¸ Setup and Run with Docker

### âœ… 1. Prepare Required Files

* `solution.py`, `dockerfile`, `requirements.txt`
* `input/input.json`: Your input metadata file (see format below)
* All referenced PDFs/HTMLs inside `input/`
* Required models inside `models/`

Example folder:

```
models/
â”œâ”€â”€ tinyllama/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
â””â”€â”€ bge-small-en-v1.5/*

input/
â”œâ”€â”€ input.json
â””â”€â”€ FranceTravelGuide.pdf
```

---

### ğŸ³ 2. Build Docker Image

```bash
docker build -t adobe-1b-solution .
```

Make sure your `dockerfile` includes:

```Dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY . .
RUN pip install --no-cache-dir -r requirements.txt
CMD ["python", "solution.py"]
```

---

### â–¶ï¸ 3. Run Container

```bash
# Windows (PowerShell)
docker run --rm -v ${PWD}:/app adobe-1b-solution

# macOS/Linux
docker run --rm -v $PWD:/app adobe-1b-solution
```

---

### ğŸ“„ 4. Output Location

Final result will be saved at:

```
output/output.json
```

It contains:

* Ranked document sections relevant to the persona
* Refined summaries for each section

---

## ğŸ§  Sample `input/input.json`

```json
{
  "challenge_info": {
    "challenge_id": "round_1b_002",
    "test_case_name": "travel_planner"
  },
  "persona": {
    "role": "Travel Planner"
  },
  "job_to_be_done": {
    "task": "Plan a trip of 4 days for a group of 10 college friends."
  },
  "documents": [
    {
      "filename": "FranceTravelGuide.pdf",
      "title": "France Travel"
    }
  ]
}
```

---

## âœ… Recap Commands

```bash
# Build
docker build -t adobe-1b-solution .

# Run (mount current directory to container)
docker run --rm -v ${PWD}:/app adobe-1b-solution
```

---

## ğŸ“¦ Python Dependencies

Make sure `requirements.txt` includes:

```txt
langchain-community
langchain-huggingface
llama-cpp-python
transformers
sentence-transformers
torch
faiss-cpu
PyPDF2
pypdf
unstructured
huggingface-hub
numpy
pathlib
```

---

## ğŸ“ Notes

* Models must be downloaded manually and stored in `models/`
* Uses `TinyLLaMA` (GGUF format) for LLM inference
* Designed for **offline CPU** execution
* All results saved in structured `JSON` format
* âš ï¸ **Model files are pushed using Git LFS**

  * Clone the repo like this:

    ```bash
    git lfs install
    git clone https://github.com/DarkMatter1217/Adobe_1B.git
    ```
